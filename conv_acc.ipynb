{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_acc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMixWhjQeAbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44vQQA4EeLzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import cv2, math\n",
        "from imageio import imsave\n",
        "from scipy import signal\n",
        "from matplotlib import pyplot as plt\n",
        "from numba import njit, cuda, vectorize, guvectorize, stencil\n",
        "from numba import prange\n",
        "import cupy as cp\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9hb8-xBuCX4",
        "colab_type": "text"
      },
      "source": [
        "Install additional profiling tools: line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC_vlhDw9_ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install line_profiler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3lCxgkd-8tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext line_profiler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5qsdtKFklQ_",
        "colab_type": "text"
      },
      "source": [
        "In a normal convolution , we **slide** the input filter spatially over the input image and compute **dot products**. Thus, it can be considered as a series of **elementwise multiplications** of filter with overalapping regions of inputs followed by an **addition** operation. Here, we limit ourselves to the most basic version of **2D** convolution on a grayscale image with a predefined **filter** with one **channel**(depth =1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LxujCcLeM8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@njit\n",
        "def convolution(mat, fil, res, stride=1):\n",
        "    '''\n",
        "    Performs convolution on a 2d square matrix.\n",
        "    Expects a kernel with same height and width.\n",
        "\n",
        "    '''\n",
        "    row_range = col_range = len(mat)-len(fil)+1\n",
        "    ker_sz=len(fil)\n",
        "    for row in range(row_range):\n",
        "        for col in range(col_range):\n",
        "\n",
        "            # Elementwise multiplication of input submat with kernel \n",
        "            submat = mat[row:row+ker_sz, col:col+ker_sz]\n",
        "            # Sum up the values and add bias\n",
        "            respix = np.sum(submat*fil)\n",
        "\n",
        "            res[row, col] = respix\n",
        "    return res\n",
        "\n",
        "@njit(parallel=True)\n",
        "def convolution_parallel(mat, fil, res, stride=1):\n",
        "    '''\n",
        "    Performs convolution on a 2d square matrix.\n",
        "    Expects a kernel with same height and width.\n",
        "\n",
        "    '''\n",
        "    row_range = col_range = len(mat)-len(fil)+1\n",
        "    ker_sz=len(fil)\n",
        "    for row in prange(row_range):\n",
        "        for col in prange(col_range):\n",
        "\n",
        "            # Elementwise multiplication of input submat with kernel \n",
        "            submat = mat[row:row+ker_sz, col:col+ker_sz]\n",
        "            # Sum up the values and add bias\n",
        "            respix = np.sum(submat*fil)\n",
        "\n",
        "            res[row, col] = respix\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CuaQjrChpUA",
        "colab_type": "text"
      },
      "source": [
        "Read a grayscale image of size 1024*1024 to work with 2D convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFyMt-4GenvN",
        "colab_type": "code",
        "outputId": "dbb9c502-0d7b-4b54-e9be-07a1c2a2bced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Input image\n",
        "img=cv2.imread('/content/lena.png',cv2.IMREAD_GRAYSCALE)\n",
        "print(\"Image shape: {}\".format(img.shape))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape: (1024, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP8IPczTh0wh",
        "colab_type": "text"
      },
      "source": [
        "For simplicity, use stride=1 and set kernel width=kernel height. Now padding = (F - 1)//2 , where F is the filter size. Let's fix kernel size as 3x3 and then the padding value=1.\n",
        "\n",
        "The general formula to find output size for convolution is:\n",
        "OS = [(W−K+2P)/S]+1, where W is input, K is kernel, P is padding and S is stride. Now, in our case OS = [(1024-3 + 2*1)/1]=1023. In general if kernel and input is square and stride=1, output shape = W -1 if kernel-size is odd and W if it's even.\n",
        "\n",
        "Let's set kernel size = (3x3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gSPpHz3ymPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bf30722c-a2bd-4b9f-f94b-66397303365c"
      },
      "source": [
        "#Input filter\n",
        "fil= np.random.rand(3, 3).astype(np.float32)\n",
        "print(\"Filter shape: {0}, Filter type: {1}\".format(fil.shape,fil.dtype))\n",
        "\n",
        "# Pad input if needed (value: 0, width: pad)\n",
        "pad = (len(fil)-1)//2\n",
        "img_pad = np.pad(img, pad, 'constant').astype(np.float32)\n",
        "print(\"Image shape: {0}, Image type: {1} \".format(img_pad.shape, img_pad.dtype))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filter shape: (3, 3), Filter type: float32\n",
            "Image shape: (1026, 1026), Image type: float32 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNpnG5T2aQqY",
        "colab_type": "text"
      },
      "source": [
        "Since we add padding, the output after convoltuion should have same size as original input image (i.e same padding), since it prevents overall shrinking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0pFJnC-gMeH",
        "colab_type": "code",
        "outputId": "26a3c00f-8922-4687-9162-cd170b578a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Output result (placeholder)\n",
        "res = np.zeros(img.shape, dtype=np.float32)\n",
        "print(\"Output shape: {0}, Output type: {1}\".format(res.shape, res.dtype))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output shape: (1024, 1024), Output type: float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exq4R1_mwJOP",
        "colab_type": "text"
      },
      "source": [
        "Let's run the convolution function without numba.The decorator 'njit' is used to compile the function just-in-time, in no=python mode. The original python function can be accessed as 'convolution.py_func'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXjzkwN5AOUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convolve function without numba\n",
        "results = convolution.py_func(img_pad, fil,res=res)\n",
        "%timeit convolution.py_func(img_pad, fil,res=res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI3LQUmjcFQT",
        "colab_type": "text"
      },
      "source": [
        "Run the line profiler to find bottlenecks in the convolution function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA6nR-z_MDVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Profile with line-profiler (use milli sec)\n",
        "%lprun -u 1e-3 -f convolution.py_func convolution.py_func(img_pad,fil,res=res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOfjR0gzMHUI",
        "colab_type": "text"
      },
      "source": [
        "According to the results from line profiler, about **82%**(i.e 9.5/11.5 ms) of execution time is spent on the line: **respix = np.sum(submat*fil)**.\n",
        "\n",
        "Now let's **speed-up** the convoultion using numb-jit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xib6fdmKwESP",
        "colab_type": "code",
        "outputId": "84fd75a5-bb77-4c41-b71a-d1122120dbaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# convolve function with numba\n",
        "results = convolution(img_pad, fil,res=res) # Once the function is compiled and executed, the output is saved.\n",
        "%timeit convolution(img_pad, fil,res=res) # Mesure average execution time"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 118 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m43XnKrZ1Z-b",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "We can add an additional argument 'parallel=True' to the jit decorator to exploit potential parallelization. In this case we also replace range with numba 'prange'  within the for loops of the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9LOkaj70yYi",
        "colab_type": "code",
        "outputId": "3069ece0-961e-490b-f012-548674db9b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# convolve function with numba using parallel \n",
        "results = convolution_parallel(img_pad, fil,res=res)\n",
        "%timeit convolution_parallel(img_pad, fil,res=res)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 26.5 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCU64g1CJy9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run parallel diagnostics for profiling \n",
        "convolution_parallel.parallel_diagnostics(level=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAyq2dNxmvT",
        "colab_type": "text"
      },
      "source": [
        "Let's compute convolution using the scipy signal module. Here, we have to modify the kernel to adapt to the linear convolution behaviour in scipy to get the same results (Actually the ML convolution is correlation; not a real convolution). For this purpose, we flip the kernel along both the axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3utZfZIbqPom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scipy convolve\n",
        "results_scipy = signal.convolve2d(img_pad, np.flip(fil, (0,1)), 'valid')\n",
        "%timeit signal.convolve2d(img_pad, np.flip(fil, (0,1)), 'valid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm6mS_IzZvcz",
        "colab_type": "text"
      },
      "source": [
        "Lets' compare the outputs of the convolution using python-numba and scipy. Since all the python-numba used the same code(with differetn decorators), it would be sufficent to compare one numba version version output with scipy version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RrR0UlltSwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify outputs\n",
        "print(\"Verification correct: \", np.allclose(results, results_scipy))\n",
        "\n",
        "# Plot outputs\n",
        "plt.imshow(results)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JtmDo5hd5qT",
        "colab_type": "text"
      },
      "source": [
        "**Results:-**\n",
        "\n",
        "Here is the **summary** of convolution operation performance in terms of **execution time**.\n",
        "\n",
        "```\n",
        "1.   Normal Convolution              - 6260 ms     \n",
        "2.   Numba-JIT Convolution           -  125 ms\n",
        "3.   Scipy Signal Convolution        -   42 ms\n",
        "4.   Numba-JIT Parallel Convolution   -  28 ms\n",
        "```\n",
        "\n",
        "**Note:-**                                                                        \n",
        "*   **CPU**: Intel(R) Xeon(R) CPU @ 2.30GHz, Dual-Core \n",
        "*   **Inputs**: 2D float32 matrices (i.e image: 1024x1024, filter: 3x3) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJVTCkwEtaos",
        "colab_type": "text"
      },
      "source": [
        "**Optimization of convolution algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyDAHtY5wdae",
        "colab_type": "text"
      },
      "source": [
        "An **efficient** implementation of convolution usually involves a transformation of input matrix and filters into a unrolled or expanded layout, such that the **convolution** operation can then be performed as a **matrx multiplication** on the modified inputs.This transformation is usually referred to as '**im2col**'. Even though this makes the overall **convoltuion faster**; it significantly **increases the memory** requirement of the convolution process. Finally, matrix-multiplication outputs are converted back to the original input matrix representation using the **col2im** operation (i.e inverse of im2col)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCAh_wr3-ETz",
        "colab_type": "text"
      },
      "source": [
        "**Note:** For simplicity, we set parameters like batch size, stride and number of channels as 1 and we perform padding as a pre-processing step to convolution operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akltJ3BYti_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@njit\n",
        "def im2col_2d(mat, fil, res=1):\n",
        "    '''\n",
        "    Expects input and kernel to be square shape\n",
        "    Returns : im2col view with shape - (ker_sz,ker_sz,img_sz,img_sz)\n",
        "    '''\n",
        "    # Parameters\n",
        "    row_range = col_range = len(mat)-len(fil)+1\n",
        "    ker_sz=len(fil)\n",
        "    s0, s1 = mat.strides    \n",
        "    shp = ker_sz,ker_sz,row_range,col_range\n",
        "    strd = s0,s1,s0,s1\n",
        "\n",
        "    out_view = np.lib.stride_tricks.as_strided(mat, shape=shp, strides=strd)\n",
        "    return out_view\n",
        "\n",
        "@guvectorize(['void(float32[:], float32[:,:], float32[:])'],'(n),(n,p)->(p)',target='parallel')\n",
        "def matmulcore_cpu(A, B, C):\n",
        "    n = len(A)\n",
        "    n, p = B.shape\n",
        "    for k in prange(n):\n",
        "        for j in prange(p):\n",
        "            C[j] += A[k] * B[k, j] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNXR_E6Jbr7c",
        "colab_type": "text"
      },
      "source": [
        "First let's try out the classic numpy matmul(or dot in 2D) method along with im2col and col2im."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auoSokpffA3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute and save the result\n",
        "col_mat=im2col_2d(img_pad, fil,res=1) # im2col reshape\n",
        "mul_mat=np.matmul(fil.reshape(1,len(col_mat)*len(col_mat)), col_mat.reshape(len(col_mat)*len(col_mat),-1)) # matrix multiplication\n",
        "results_numpy=mul_mat.reshape(len(img),len(img)) # col2im reshape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdXGhYurz-B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mesure execution time with timeit\n",
        "def numpy_conv(img_pad, fil,res=1):\n",
        "    col_mat=im2col_2d(img_pad, fil,res=1) # im2col reshape\n",
        "    mul_mat=np.matmul(fil.reshape(1,len(col_mat)*len(col_mat)), col_mat.reshape(len(col_mat)*len(col_mat),-1)) # matrix multiplication\n",
        "    results_numpy=mul_mat.reshape(len(img),len(img)) # col2im reshape\n",
        "\n",
        "%timeit numpy_conv(img_pad, fil,res=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybU4loyBh2vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Profile with line-profiler (use milli sec)\n",
        "%lprun -u 1e-3 -f numpy_conv numpy_conv(img_pad, fil,res=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkSPEKYchtju",
        "colab_type": "text"
      },
      "source": [
        "From the line profiler log, it is clear that **99.7%** of the time is used in **np.matul**, i.e **im2col and col2im** operations have **negligible** execution time (**Caching** could also be the reason).\n",
        "\n",
        "Now, lets execute the numba version of convolution. Here, we use a vectorized parallel version of matrix multiplication instead of built-in numpy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXtPdoWRiXDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile, execute and save the output\n",
        "C=np.zeros(len(img)*len(img), dtype=np.float32)\n",
        "col_mat=im2col_2d(img_pad, fil,res=1) # im2col reshape\n",
        "colres=matmulcore_cpu(fil.reshape(1,len(col_mat)*len(col_mat)), col_mat.reshape(len(col_mat)*len(col_mat),-1), C) # matrix multiplication\n",
        "results_matmulcpu=colres.reshape(len(img),len(img)) # col2im reshape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-iAVRJSuZT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit\n",
        "C=np.zeros(len(img)*len(img), dtype=np.float32)\n",
        "col_mat=im2col_2d(img_pad, fil,res=1) # im2col reshape\n",
        "colres=matmulcore_cpu(fil.reshape(1,len(col_mat)*len(col_mat)), col_mat.reshape(len(col_mat)*len(col_mat),-1), C) # matrix multiplication\n",
        "results_matmulcpu=colres.reshape(len(img),len(img)) # col2im reshape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDRE8XnPisyB",
        "colab_type": "text"
      },
      "source": [
        "This, time it seems buit-in **numpy function** is slightly **faster** than our numba-jitted function.\n",
        "\n",
        "Let's compare the results of numpy and numba version of the new optimized convolution algoritm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajcdmWf1prh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify outputs\n",
        "print(\"Verification CPU correct : \", np.allclose(results_numpy, results_matmulcpu))\n",
        "\n",
        "# Plot outputs\n",
        "plt.imshow(results_matmulcpu)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzPB9mU5mMCj",
        "colab_type": "text"
      },
      "source": [
        "**Results:-**\n",
        "\n",
        "Here is the performance **summary** of the optimized convolution algorithm with **im2col and col2im**.\n",
        "\n",
        "```\n",
        "1.   Numpy Im2Col Convolution               -    8 ms     \n",
        "2.   Numba-JIT Parallel Im2Col Convolution  -   13 ms\n",
        "```\n",
        "\n",
        "**Note:-**                                                                        \n",
        "*   **CPU**: Intel(R) Xeon(R) CPU @ 2.30GHz, Dual-Core \n",
        "*   **Inputs**: 2D float32 matrices (i.e image: 1024x1024, filter: 3x3) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA-4svWaiJne",
        "colab_type": "text"
      },
      "source": [
        "**Convolution in GPU: Cupy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7HpGITiASnz",
        "colab_type": "text"
      },
      "source": [
        "CuPy is an open-source matrix library accelerated with CUDA. It uses libraries like CUBLAS,CuFFT and CuDNN for acclerating matrix operations under the hood. It's interface is highly compatible with numpy and in most cases you can directly replace the numpy import with cupy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EngTy7TOpN81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@njit\n",
        "def im2col_2d(mat, fil, res=1):\n",
        "    '''\n",
        "    Expects input and kernel to be square shape\n",
        "    Returns : im2col view with shape - (ker_sz,ker_sz,img_sz,img_sz)\n",
        "    '''\n",
        "    # Parameters\n",
        "    row_range = col_range = len(mat)-len(fil)+1\n",
        "    ker_sz=len(fil)\n",
        "    s0, s1 = mat.strides    \n",
        "    shp = ker_sz,ker_sz,row_range,col_range\n",
        "    strd = s0,s1,s0,s1\n",
        "\n",
        "    out_view = np.lib.stride_tricks.as_strided(mat, shape=shp, strides=strd)\n",
        "    return out_view"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LiCR_rJEtnp",
        "colab_type": "text"
      },
      "source": [
        "Initially transform input using **im2col**. Then, **copy** the inputs from host to **device memory** i.e GPU. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIir8alOiYVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert inputs using im2col in CPU\n",
        "col_mat=im2col_2d(img_pad, fil,res=1) # im2col reshape\n",
        "\n",
        "# Load inputs and allocate output memory in GPU\n",
        "fil_GPU=cp.asarray(fil.reshape(1,len(col_mat)*len(col_mat))) # (1,9)\n",
        "mat_GPU=cp.asarray(col_mat.reshape(len(col_mat)*len(col_mat),-1)) # (9, 1048576)\n",
        "out_GPU=cp.asarray(np.zeros((len(img),len(img)), dtype=np.float32)) # (1024,1024)\n",
        "\n",
        "print(\"Output device : {0}, Data type: {1}, Shape: {2}\".format(out_GPU.device, out_GPU.dtype, out_GPU.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ3XS6c4GvKf",
        "colab_type": "text"
      },
      "source": [
        "Now you can perform **dot product** directly using cupy. Since cupy syntax is very similar to numpy, **col2im_2D** operation can be performed using **reshape** operator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij8mQDqFlieJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute dot product on GPU and reshape output(col2im_2D)\n",
        "out_GPU=cp.dot(fil_GPU,mat_GPU).reshape(1024,1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvRH3f5QiFtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Measure execution time on GPU, using cupy\n",
        "%%timeit\n",
        "out_GPU=cp.dot(fil_GPU,mat_GPU).reshape(1024,1024)\n",
        "cp.cuda.Device().synchronize() # Do explicit synchronize to prevent asynchronous execution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTVNcgu4vkoD",
        "colab_type": "text"
      },
      "source": [
        "Thus cupy(GPU) gives around 30x speed improvent compared to our previous best result of 8 ms, using numpy built-in functions. Remember, that the built-in numpy and cupy functions are highly optimized using BLAS libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E37gIm9GmRtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy results back to cpu memory\n",
        "results_cupy=cp.asnumpy(out_GPU)\n",
        "results_cupy.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2TpkxfcG71v",
        "colab_type": "text"
      },
      "source": [
        "Finally, copy the results back to host memory and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3FFqh0H37bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify outputs\n",
        "print(\"Verification Cupy correct : \", np.allclose(results, results_cupy))\n",
        "\n",
        "# Plot outputs\n",
        "plt.imshow(results_cupy)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcB0B2vWuYlc",
        "colab_type": "text"
      },
      "source": [
        "**Convolution in GPU: Numba CUDA**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSsaVts2xZPN",
        "colab_type": "text"
      },
      "source": [
        "The **CUDA JIT** is a low-level entry point to the CUDA features in Numba. It translates Python functions into **PTX code** which execute on the CUDA hardware. We can use a **cuda.jit** decorator, just like CPU, to compile our python code and make it run on GPU. However, you have to configure a hierarchy of '**grid of thread blocks**' and modify your code according to the parallel programming model prescribed by **CUDA API**, with the help of numba functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I32wPx21rP1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@njit\n",
        "def im2col_2d(mat, fil, res=1):\n",
        "    '''\n",
        "    Expects input and kernel to be square shape\n",
        "    Returns : im2col view with shape - (ker_sz,ker_sz,img_sz,img_sz)\n",
        "    '''\n",
        "    # Parameters\n",
        "    row_range = col_range = len(mat)-len(fil)+1\n",
        "    ker_sz=len(fil)\n",
        "    s0, s1 = mat.strides    \n",
        "    shp = ker_sz,ker_sz,row_range,col_range\n",
        "    strd = s0,s1,s0,s1\n",
        "\n",
        "    out_view = np.lib.stride_tricks.as_strided(mat, shape=shp, strides=strd)\n",
        "    return out_view\n",
        "\n",
        "\n",
        "@cuda.jit('(float32[:,:], float32[:,:], float32[:,:])')\n",
        "def matmulcore_gpu(A, B, C):\n",
        "\n",
        "    i, j = cuda.grid(2)\n",
        "    if i < C.shape[0] and j < C.shape[1]:\n",
        "        tmp = 0.\n",
        "        for k in range(A.shape[1]):\n",
        "            tmp += A[i, k] * B[k, j]\n",
        "        C[i, j] = tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXMDdNjT0uo9",
        "colab_type": "text"
      },
      "source": [
        "Convert the inputs using **im2col** and load it into **GPU memory**. Also, allocate global memory on **GPU** to save the **outputs** results. Once the kernel is compiled and executed, the results will be stored in this GPU memory. Now, configure the **grid and block** size according to your input shape and size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjFFf3NfuXyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert inputs using im2col in CPU\n",
        "col_mat=im2col_2d(img_pad, fil,res=1) # im2col reshape\n",
        "\n",
        "# Load inputs and allocate memory for output in GPU (globally)\n",
        "A_GPU=cuda.to_device(fil.reshape(1,len(col_mat)*len(col_mat))) # (1,9)\n",
        "B_GPU=cuda.to_device(col_mat.reshape(len(col_mat)*len(col_mat),-1)) # (9, 1048576)\n",
        "C_GPU=cuda.to_device(np.zeros((1,len(img)*len(img)), dtype=np.float32)) # (1,1048576)\n",
        "\n",
        "# Configure thread blocks for CUDA\n",
        "threadsperblock = (1, 256)\n",
        "blockspergrid_x = int(math.ceil(A_GPU.shape[0] / threadsperblock[0]))\n",
        "blockspergrid_y = int(math.ceil(B_GPU.shape[1] / threadsperblock[1]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "print(\"Blocks per gird: {}\".format(blockspergrid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1h6yhBolnQW",
        "colab_type": "text"
      },
      "source": [
        "Here we have **4096** blocks in total. Each blocks have **256** thread in them.\n",
        "So, 4096*256 = 1048576 => Output size.\n",
        "\n",
        "**Note:** Here the 2D dimension(1) is redundant, since our output and input filter are just 1D vectors in essence. The extra axis is for the sake of generalization demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5AYKg0Mrfha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute and benchmark the GPU kernel execution time\n",
        "%%timeit\n",
        "matmulcore_gpu[blockspergrid, threadsperblock](A_GPU,B_GPU, C_GPU)\n",
        "cuda.synchronize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkYIpcCf5EHz",
        "colab_type": "text"
      },
      "source": [
        "**Note:** By default, the kernel executes **asynchronously** with respect to host. So, add an explicit '**synchronize**' function to make sure that the **timeit** measures the actual **execution time** of the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGqLQleUwyv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy results back to cpu memory\n",
        "results_numba_gpu=C_GPU.copy_to_host()\n",
        "results_numba_gpu=results_numba_gpu.reshape(1024,1024) # col2im2D\n",
        "results_numba_gpu.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzI1QeK521Tt",
        "colab_type": "text"
      },
      "source": [
        "Copy the results back to **host memory** and **reshape** the ouputs(col2im_2D). Finally, **compare** the outputs and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMaXKpi7w4To",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify outputs\n",
        "print(\"Verification Numba GPU correct : \", np.allclose(results, results_numba_gpu))\n",
        "\n",
        "# Plot outputs\n",
        "plt.imshow(results_numba_gpu)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ylEvhh5_hi",
        "colab_type": "text"
      },
      "source": [
        "**Experiment:** For further **speedup** we can use techniques like **cuda streams and shared memory** etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoRE7JP7q8ic",
        "colab_type": "text"
      },
      "source": [
        "**Results:-**\n",
        "\n",
        "Here is the performance **summary** of the optimized convolution algorithm with **im2col and col2im**.\n",
        "\n",
        "```\n",
        "1.   Cupy Im2Col Convolution            -   256 μs     \n",
        "2.   Numba-JIT CUDA Im2Col Convolution  -   230 μs\n",
        "```\n",
        "\n",
        "**Note:-**                                                                        \n",
        "*   **CPU**: Intel(R) Xeon(R) CPU @ 2.30GHz, Dual-Core \n",
        "*   **GPU**: Nvidia Tesla T4, CUDA Version: 10.1 \n",
        "*   **Inputs**: 2D float32 matrices (i.e image: 1024x1024, filter: 3x3) \n",
        "*   Host-to-device/Device-to-host **memory copy time** is not included in timing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXrqy7hRQvjO",
        "colab_type": "text"
      },
      "source": [
        "**XNOR Convolution** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Din-KlwEkuL",
        "colab_type": "text"
      },
      "source": [
        "The XNOR convolution used in binary neural networks, has both the **inputs and filters in binarized format**. Using this approximate representation, we can compute convolution using **XNOR and bitcounting** operations instead of traditional MAC operations. Even though there would be a drop in accuracy, the advantages include **speed-up, memory savings and reducd power** requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrYepGkKQ3WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@njit\n",
        "def im2col_2d(mat, fil, res=1):\n",
        "    '''\n",
        "    Expects input and kernel to be square shape\n",
        "    Returns : im2col view with shape - (ker_sz,ker_sz,img_sz,img_sz)\n",
        "    '''\n",
        "    # Parameters\n",
        "    row_range = col_range = len(mat)-len(fil)+1\n",
        "    ker_sz=len(fil)\n",
        "    s0, s1 = mat.strides    \n",
        "    shp = ker_sz,ker_sz,row_range,col_range\n",
        "    strd = s0,s1,s0,s1\n",
        "\n",
        "    out_view = np.lib.stride_tricks.as_strided(mat, shape=shp, strides=strd)\n",
        "    \n",
        "    return out_view"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlvyZa35XWaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize inputs and filters randomly\n",
        "img_pad=np.random.uniform(low=-1.0, high=1.0, size=(1026,1026)).astype(np.float32)\n",
        "fil=np.random.uniform(low=-1.0, high=1.0, size=(3,3)).astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whaTjXr6MTkH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Initialy, we convert the inputs and filters with **im2col** operation and then \n",
        "**binarize** them to 0 and 1(or 1 & -1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6BI82oLK9ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_mat=im2col_2d(img_pad, fil,res=1) \n",
        "col_mat = col_mat.reshape(len(col_mat)*len(col_mat),-1) #(9, 1048576)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "635wXVYzSRfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# im2col reshape\n",
        "col_mat=im2col_2d(img_pad, fil,res=1) \n",
        "col_mat = col_mat.reshape(len(col_mat)*len(col_mat),-1) #(9, 1048576)\n",
        "fil_mat=fil.reshape(1,len(fil)*len(fil)) # (1, 9)\n",
        "\n",
        "# Binarize col_mat and fil_mat\n",
        "bin_col=np.uint8(col_mat>0)\n",
        "bin_fil=np.uint8(fil_mat>0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__z8_mJQObi0",
        "colab_type": "text"
      },
      "source": [
        "Here is the comple **formula for XNOR** convolution:-\n",
        "\n",
        "\n",
        "*   **W**  -  real valued weights (filter)\n",
        "*   **I**  -  real valued input tensor\n",
        "*   **\\***  -  normal convolution operation\n",
        "\n",
        "    **`I ∗ W ≈ H ⊛ B ⊙ αK`**\n",
        "\n",
        "* **H** – input binary tensor (the sign of I)\n",
        "* **B** – binary weights (the sign of W)\n",
        "* **⊛** is a convolution using only XNOR and bit-counting\n",
        "* **α** – real valued scale factors (the average of |W|)\n",
        "* **K** – real valued scale factors of input conv-windows(absolute values)\n",
        "* **⊙** is elementwise multiplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJQweS0sXzeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate scale factors K and alpha\n",
        "K=np.mean(np.abs(col_mat),axis=0) # float32 (1048576,)\n",
        "alpha=np.mean(np.abs(fil_mat)) # foat32 ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVGCyP2_jgWs",
        "colab_type": "text"
      },
      "source": [
        "Compare execution time for calculation of **K**, between python and numba versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-TKuKehd_Ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parallel mean on array after im2col with reduction\n",
        "z=np.zeros(1048576,dtype=np.float32)\n",
        "@njit(parallel=True)\n",
        "def fmean(arr,z):\n",
        "    for i in prange(1048576):\n",
        "      for j in prange(9):\n",
        "       z[i]+=np.abs(arr[j,i])\n",
        "    return z/9\n",
        "\n",
        "\n",
        "# Parallel mean on array after im2col with vectorization\n",
        "@guvectorize(['(float32[:], float32[:])'],'(n)->()', target='parallel')\n",
        "def guvec_avg(array, out):\n",
        "    acc = 0\n",
        "    n = len(array.T)\n",
        "    for val in array:\n",
        "        acc += np.abs(val)\n",
        "    out[0] = acc/n\n",
        "\n",
        "\n",
        "# Parallel mean on array before im2col with stencil\n",
        "@stencil\n",
        "def kernell(a):\n",
        "    return (abs(a[0, 0]) + abs(a[0, 1]) + abs(a[0, 2]) \n",
        "          + abs(a[1, 0]) + abs(a[1, 1]) + abs(a[1, 2])\n",
        "          + abs(a[2, 0]) + abs(a[2, 1]) + abs(a[2, 2]))/9\n",
        "\n",
        "@njit(parallel=True)\n",
        "def stencil_avg(img_pad):\n",
        "    return kernell(img_pad)\n",
        "\n",
        "# Get results\n",
        "avg=stencil_avg(np.abs(img_pad))\n",
        "stencil_version=avg[:1024,:1024].reshape(-1)\n",
        "im2col_version=fmean(col_mat,z)\n",
        "guvec_version = guvec_avg(col_mat.T)\n",
        "pyvecd_version=np.mean(np.abs(col_mat),axis=0,dtype=np.float32)\n",
        "\n",
        "# Compare execution time\n",
        "%timeit stencil_avg(img_pad)\n",
        "%timeit fmean(col_mat,z)\n",
        "%timeit np.mean(np.abs(col_mat),axis=0,dtype=np.float32)\n",
        "%timeit guvec_avg(col_mat.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OmvU_LSjrds",
        "colab_type": "text"
      },
      "source": [
        "**Note:** Both @vectorize and @guvectorize parallelize on the **broadcast dimensions** in a ufunc/gufunc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrIN96d-lCev",
        "colab_type": "text"
      },
      "source": [
        "Perform **xnor** betwen binary filter and binary input array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQYEhAUWcoFx",
        "colab_type": "code",
        "outputId": "8913e7b6-588c-447f-cb32-9fc176374cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate xnor between col and fil\n",
        "out_xnor=np.logical_not(np.logical_xor(bin_fil.reshape((-1,1)),bin_col))\n",
        "print(\"Out shape: {}, Out type: {}\".format(out_xnor.shape, out_xnor.dtype))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Out shape: (9, 1048576), Out type: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLpF1LzTSmPG",
        "colab_type": "text"
      },
      "source": [
        "After performing XNOR operation, we need to apply **bitcount** over individual columns of binary numbers. The final result is obtained using the formula,\n",
        "\n",
        "**`res = 2*popcount - num(bits)`**\n",
        "\n",
        "Here **popcoun**t refers to the number of setbits of binary input. Since we have 0's and 1's we can just **sum** up the values across the columns for computing popcount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8jZbQkkn84r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate bitcount using formula b = 2*popcount - num(bits)\n",
        "out = 2*(np.sum(out_xnor,axis=0)) - len(out_xnor)\n",
        "\n",
        "# Multiply result with scaling factor\n",
        "res = out*K*alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsArDQD1U4lE",
        "colab_type": "text"
      },
      "source": [
        "Now, **multiply** the result with **K and alpha elementwis**e for **scaling** the output values and  finally **reshape** the output according to the original (calculated) output shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAu_1Sd7q6tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the output \n",
        "xnor_result= res.reshape((1024,1024))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNa5lbHuWGnu",
        "colab_type": "text"
      },
      "source": [
        "**NB**: Here, we limit ourselves to **2D inputs** for sake of simplicity. In general we have to take **averages** across all the **channels** for calculating **K** values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wUtn3WiwnXx",
        "colab_type": "text"
      },
      "source": [
        "**XNOR Convolution With Bitpacking**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6CjI5CUYv0u",
        "colab_type": "text"
      },
      "source": [
        "In the previous example, even though we converted our inputs to **binray format** (0/1), they were still occupying **64 bits** (32 bits if inputs are float32) in memory. We could also have converted the binary inputs of 0's and 1's into **UINT8** format for saving memory.\n",
        "\n",
        "Here, we use another technique called **bitpacking** for compressing our inputs in memory. We **group** together consecutive **64** elements (0 or 1) into a single **UINT64** number. In our transformed input matrix (bin_col: (1048576, 9)), we group **consecutive rows** together such that new row length approximately becomes 64.\n",
        "\n",
        "`Reshape => 7 consecutive rows of 9 elements = 63 bits or elements = a new row.`\n",
        "\n",
        "Initially, we have to **pad input** mat with zero row vectors (i.e **3** nos,since 1048576mod7=4) to complete the **last batch** or group of 7 rows. For each row **pad** the remaining positions to make its length 64(i.e add a zero at the end of each row). In the case of input filter, we **repeat** the kernel **7 times** and then pad it with a zero. Now, the kernel row vector and each rows of the input have 64 binary elements. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z6RmzObp4xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get binarized inputs \n",
        "bin_fil=bin_fil.copy() # shape: (1, 9)\n",
        "bin_col=bin_col.T.copy() # shape: (1048576, 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmID8qaByHv3",
        "colab_type": "text"
      },
      "source": [
        "Our aim is to pack a group of **consecutive elements** (each element UINT8/Float/Bool of 0 or 1) into a **single 64 bit UINT64** element, such that our **kernel** becomes **single number**(0D) and **input** a **1D vector**, both of type UINT64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow1ppQPnx1Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bitpack(bin_fil,bin_col):\n",
        "  \n",
        "  # Bitpack filter\n",
        "  a=bin_fil.repeat(7,axis=0)\n",
        "  a_64=np.append(a, [0]) # (64,) \n",
        "  fil_pack=np.packbits(a_64).view(np.uint64).byteswap()\n",
        "  \n",
        "  # Bitpack image\n",
        "  row_pad=np.zeros((3,9),dtype=np.uint8) \n",
        "  b=np.concatenate([bin_col,row_pad],axis=0) # pad col end(1048576 to 1048579)\n",
        "  b=b.reshape((-1,63))\n",
        "\n",
        "  col_pad=np.zeros((b.shape[0],1),dtype=np.uint8)\n",
        "  b=np.concatenate([b,col_pad],axis=1) # pad row end(63 to 64)\n",
        "  col_pack=np.packbits(b).view(np.uint64).byteswap()\n",
        "  \n",
        "  return fil_pack, col_pack\n",
        "\n",
        "# Execute and save the results\n",
        "fil_pack,col_pack=bitpack(bin_fil,bin_col)\n",
        "\n",
        "print(\"fil_pack: shape= {0}, type= {1}\".format(fil_pack.shape, fil_pack.dtype))\n",
        "print(\"col_pack: shape= {0}, type= {1}\".format(len(col_pack),col_pack.dtype))\n",
        "\n",
        "# Compute average execution time\n",
        "%timeit bitpack(bin_fil,bin_col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA9eGmcS14OB",
        "colab_type": "text"
      },
      "source": [
        "**Note**: Reshaping a large **non-contiguous** array(ie non-C_CONTIGUOUS) consumes significant amount of time compared to it's contiguous version, so verify them using **flags** properties of array. This situation sometimes arises if we use reshape on a **view or transposed** version of a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgTVFktzcp6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comapre numpy bitpack wiht numba bitpack\n",
        "test = np.random.randint(0, 2, (149797, 64),dtype=np.uint8)\n",
        "@njit(parallel=True)\n",
        "def shifting(bitlist): \n",
        "    rows,cols=149797,64\n",
        "    out=0\n",
        "    z=np.zeros(rows,dtype=np.uint64)\n",
        "    for i in prange(rows):\n",
        "      for bit in range(cols):\n",
        "         z[i] = (z[i] * 2) + bitlist[i,bit] \n",
        "\n",
        "    return z\n",
        "\n",
        "%timeit shifting(test)\n",
        "%timeit np.packbits(test).view(np.uint64).byteswap()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-j7ThSGdjzR",
        "colab_type": "text"
      },
      "source": [
        "**Note**: The **performance** depends upon your hardware(more cores,clocks speed => numba parallel better) and **library versions**(highly optimized numpy), input size etc. Using a **12 core** machine, i got **10x** speed-up for numba version, in comparison to numpy pack-bits. Anyway, in google **colab with dual core cpu** and default numpy, the pure **numpy version** seems to be faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvlOUCjaue-u",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Finally we perform **bitwise XNOR** of kernel across all elements of input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NGBdusccbjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform XNOR between inputs\n",
        "z=np.bitwise_not(np.bitwise_xor(fil_pack,col_pack))\n",
        "%timeit np.bitwise_not(np.bitwise_xor(fil_pack,col_pack))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xRwaDRYg794",
        "colab_type": "text"
      },
      "source": [
        "Now, **z** is an **UINT64** array of size **149797**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwyNT6hPEc8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classic C-Style bit-count for unpacking\n",
        "@njit\n",
        "def bit_count(n):\n",
        "    \"\"\"Return the number of bits set to 1 in the integer number 'n'.\n",
        "       This is called the Hamming weight or the population count of 'n'.\n",
        "       The algorithm performs as many iterations as there are set bits.\n",
        "       Argument 'n' must be non-negative'\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    while n:\n",
        "        n &= n - np.uint64(1)\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# Nine-bit extractor for unpacking\n",
        "@njit\n",
        "def bit_ext():\n",
        "  '''Compute and save numbers for extracting nine bits\n",
        "     from binary representation of a UINT64 number.\n",
        "  '''\n",
        "\n",
        "  # Initialize bit extractor\n",
        "  bits=np.zeros(7,dtype=np.uint64)\n",
        "  ext9=18410715276690587648# FF800000\n",
        "  for i in range(0,7):\n",
        "    pos=i*9\n",
        "    bits[i]=ext9 >> pos\n",
        "  return bits\n",
        "\n",
        "# Get bit extractor\n",
        "bits=bit_ext()\n",
        "\n",
        "# Compute average execution time\n",
        "%timeit bit_ext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFWi-jqHqe74",
        "colab_type": "text"
      },
      "source": [
        "After performing XNOR, we have to **unpack** the result from the individual elements of the array.  Each **64 bit element** has to be unpacked to **7 float numbers**(7*9+ 1 pad =64), using the unpack function. \n",
        "\n",
        "Here,for each consecutive **nine bits**, we calculate the output using the aforementioned **bitcount formula**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHPJqT8X18ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@njit\n",
        "def unpack(z):\n",
        " '''input: int64\n",
        "    output: float array of length 7 (7*9 packs)\n",
        " '''\n",
        " fout=np.zeros((1048579),dtype=np.float32)\n",
        " num=np.zeros((7),dtype=np.float32)\n",
        "\n",
        " for i in range(0,len(z)):\n",
        "   for j in range(0,7):\n",
        "     popcount=bit_count(z[i] & bits[j])\n",
        "     num[j]=2*popcount - 9\n",
        "   pos=i*7  \n",
        "   fout[pos:pos+7]=num\n",
        "   \n",
        " return fout\n",
        "\n",
        "# Unpack and save result\n",
        "final=unpack(z)\n",
        "\n",
        "# Compute average executin time\n",
        "%timeit unpack(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDkz_i3k2EBz",
        "colab_type": "text"
      },
      "source": [
        "Flatten the output and remove the **last 3 elements** corresponding to padded zero row vectors. Now, **multiply** the output with **K and alpha** elementwise and **reshape** the output according to the calculated output shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_thRl3HqFBja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale the output using K and alpha \n",
        "@njit\n",
        "def scale(mat, K, alpha):\n",
        "  scale=mat*K*alpha\n",
        "  return scale\n",
        "\n",
        "# Scale and reshape output\n",
        "xnor_bitpack=scale(final[0:-3],K,alpha).reshape((1024,1024))\n",
        "\n",
        "# Compute average execution time\n",
        "%timeit scale(final[0:-3],K,alpha).reshape((1024,1024))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC5Uwd6K2fpV",
        "colab_type": "text"
      },
      "source": [
        "Verify the results by **comparing** it with previous outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6I8lxwiWkls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Verification bitpack correct : \", np.allclose(xnor_bitpack, xnor_result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9z4pQbKs10c",
        "colab_type": "text"
      },
      "source": [
        "**Results:-**\n",
        "\n",
        "Here is the performance summary of the optimized convolution algorithm with **xnor and bitpacking**.\n",
        "\n",
        "```\n",
        "Im2col   : 4.96 ms\n",
        "Binarize : 4.86 ms\n",
        "K-alpha  : 2.70 ms\n",
        "Bitpack  : 4.53 ms\n",
        "XNOR     : 0.32 ms\n",
        "Bitunpack: 1.75 ms\n",
        "Scale    : 0.56 ms\n",
        "------------------\n",
        "i.e Total time = 20 ms\n",
        "```\n",
        "\n",
        "**Note:-**                                                                        \n",
        "*   **CPU**: Intel(R) Xeon(R) CPU @ 2.30GHz, Dual-Core \n",
        "*   **Inputs**: 2D float32 matrices (i.e image: 1024x1024, filter: 3x3)\n",
        "*   **Parallelization** speed-up depends on hardware and data size.\n",
        "*   **Operations** on non-contiguous array consumes significant amount of time.\n",
        "*   **Im2col and binarization** seems to be the current bottlenecks.\n",
        "*   Need **finer control** over memory (bit-level and hierarchy).\n",
        "*   Need **generalization** and lesser overheads.\n",
        "*   **Execution time** seems to vary between different colab runtime's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abWZrJC-hTOy",
        "colab_type": "text"
      },
      "source": [
        "**Numba: Stencil Convoltuion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhzMLHbuADpa",
        "colab_type": "text"
      },
      "source": [
        "Stencils are a common computational **pattern** in which array elements are updated according to some fixed pattern called the stencil kernel. Each element produces an output that is the **dot product** of the kernel and the 3x3 subarray neighbourhood. Using the **parallel-jit** option, we can execute the whole process in parallel with **numba stencils**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_DPkhgAMQAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input image\n",
        "img=cv2.imread('/content/lena.png',cv2.IMREAD_GRAYSCALE)\n",
        "print(\"Image shape: {}\".format(img.shape))\n",
        "\n",
        "#Input filter\n",
        "fil= np.random.rand(3, 3).astype(np.float32)\n",
        "print(\"Filter shape: {0}, Filter type: {1}\".format(fil.shape,fil.dtype))\n",
        "\n",
        "# Pad input if needed (value: 0, width: pad)\n",
        "pad = (len(fil)-1)//2\n",
        "img_pad = np.pad(img, pad, 'constant').astype(np.float32)\n",
        "print(\"Image shape: {0}, Image type: {1} \".format(img_pad.shape, img_pad.dtype))\n",
        "\n",
        "# Output result (placeholder)\n",
        "res = np.zeros(img.shape, dtype=np.float32)\n",
        "print(\"Output shape: {0}, Output type: {1}\".format(res.shape, res.dtype))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yoMnPUqbNv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parallel conv on image with stencil\n",
        "@stencil\n",
        "def kernel_conv(a):\n",
        "    return (a[0, 0]*fil[0,0] + a[0, 1]*fil[0,1] + a[0, 2]*fil[0,2] \n",
        "          + a[1, 0]*fil[1,0] + a[1, 1]*fil[1,1] + a[1, 2]*fil[1,2]\n",
        "          + a[2, 0]*fil[2,0] + a[2, 1]*fil[2,1] + a[2, 2]*fil[2,2])\n",
        "\n",
        "@njit(parallel=True)\n",
        "def stencil_conv(img_pad):\n",
        "    return kernel_conv(img_pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ek6eH2qfq-K",
        "colab_type": "text"
      },
      "source": [
        "**Note:** Currently the kernel is accessed as a global variable inside stencil."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WByYGq5wWc76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare stencil convolution with scipy convolution\n",
        "stencil_cpu=stencil_conv(img_pad)[:1024,:1024]\n",
        "scipy_res = signal.convolve2d(img_pad, np.flip(fil.copy(), (0,1)), 'valid')\n",
        "\n",
        "%timeit stencil_conv(img_pad)\n",
        "np.allclose(stencil_cpu,scipy_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkrK8plRG3r6",
        "colab_type": "text"
      },
      "source": [
        "Here is the corresponding **GPU** version of the code, using generic stencils. Note that we need to configure the **grid of thread blocks** before running the kernels in GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4qaEvu85xke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@cuda.jit\n",
        "def convolve_gpu(x, k, out):\n",
        "    i, j = cuda.grid(2)\n",
        "    n, m = x.shape\n",
        "    if i < out.shape[0] and j < out.shape[1]:\n",
        "        out[i, j] = (x[i, j]  *k[0, 0] + x[i, j+1]  *k[0, 1]  + x[i, j+2]  *k[0, 2] +\n",
        "                     x[i+1, j]*k[1, 0] + x[i+1, j+1]*k[1, 1]  + x[i+1, j+2]*k[1, 2] +\n",
        "                     x[i+2, j]*k[2, 0] + x[i+2, j+1]*k[2, 1]  + x[i+2, j+2]*k[2, 2])\n",
        "\n",
        "\n",
        "# Allocate memory for inputs and outputs\n",
        "fil_gpu=cp.asarray(fil)\n",
        "mat_gpu=cp.asarray(img_pad)\n",
        "out_gpu=cp.asarray(np.zeros(img.shape, dtype=np.float32))\n",
        "\n",
        "# Configure the grid of thread blocks\n",
        "threadsperblock = (32,32)\n",
        "blockspergrid_x = math.ceil(mat_gpu.shape[0] / threadsperblock[0])\n",
        "blockspergrid_y = math.ceil(mat_gpu.shape[1] / threadsperblock[1])\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "# Execute the convolution kernel\n",
        "convolve_gpu[blockspergrid, threadsperblock](mat_gpu, fil_gpu ,out_gpu)\n",
        "\n",
        "# Copy the results back to host and compare results\n",
        "stencil_gpu = cp.asnumpy(out_gpu)\n",
        "np.allclose(stencil_cpu,stencil_gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je6DgOXJ7iaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Measure execution time for gpu-stencil\n",
        "%%timeit \n",
        "convolve_gpu[blockspergrid, threadsperblock](mat_gpu, fil_gpu ,out_gpu)\n",
        "cuda.synchronize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yBPY74II7Xm",
        "colab_type": "text"
      },
      "source": [
        "**Results:-**\n",
        "\n",
        "Here is the performance **summary** of the convolution algorithm with **stencils**.\n",
        "\n",
        "```\n",
        "1.   Numba CPU Stencil  -   3620 μs     \n",
        "2.   Numba GPU Stencil  -    740 μs\n",
        "```\n",
        "\n",
        "**Note:-**                                                                        \n",
        "*   **CPU**: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
        "*   **GPU**: Nvidia Tesla P100, CUDA Version: 10.1 \n",
        "*   **Inputs**: 2D float32 matrices (i.e image: 1024x1024, filter: 3x3) \n",
        "*   Host-to-device/Device-to-host **memory copy time** is not included in timing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG-jswcq93kv",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's see performance of **tensorflow** convolution ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dygnadFiu8oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_layer = tf.keras.layers.Conv2D(1, 3)\n",
        "\n",
        "@tf.function\n",
        "def conv_fn(image):\n",
        "  return conv_layer(image)\n",
        "\n",
        "image = tf.zeros([1, 1026, 1026, 1]) # NHWC\n",
        "# warm up\n",
        "res=conv_fn(image)\n",
        "\n",
        "print(\"Tensorflow conv:\", timeit.timeit(lambda: conv_fn(image), number=1000), \"ms per loop\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1JmqKx2E-Am",
        "colab_type": "text"
      },
      "source": [
        "It seem to be around **1 ms** in GPU !!!\n",
        "\n",
        "It surely is close to our **best results**(stand-alone) so far, considering all the potential tensorflow **overheads** ..."
      ]
    }
  ]
}